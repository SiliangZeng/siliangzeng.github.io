<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Siliang Zeng</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">MENU</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="awards.html">Awards</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Siliang Zeng</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photo.jpeg" alt="" width="200px" />&nbsp;</td>
<td align="left"><p><br /> <a href="https://cse.umn.edu/ece">Electrical and Computer Engineering</a>, <br />
<a href="https://twin-cities.umn.edu/">University of Minnesota, Twin Cities</a><br />
<a href="https://scholar.google.com/citations?user=IfqsDyYAAAAJ&amp;hl=en">Google Scholar</a>, <a href="https://www.dropbox.com/scl/fi/jnlnhq6w1wfyc4f4yxyqh/Siliang-CV.pdf?rlkey=wd72nae850vdkq3pjokw0nmdr&amp;st=oc5rsxcv&amp;dl=0">CV</a> <br />
Email: zeng0176 <a href="at">at</a> umn (dot) edu</p>
</td></tr></table>
<h2>About me</h2>
<p>I am a fifth year PhD student at <a href="https://twin-cities.umn.edu/">University of Minnesota</a>, advised by <a href="https://people.ece.umn.edu/~mhong/mingyi.html">Prof. Mingyi Hong</a>. <br />
Before moving to Minnesota, I received the B.S. in Statistics from <a href="https://www.cuhk.edu.cn/en">The Chinese University of Hong Kong, Shenzhen</a> in 2020. <br /></p>
<p>In my research, I am always thinking about how to design practical algorithms and systems for training RL agents to solve sequential decision-making <br />  
problems under uncertainty. My recent research interests focus on Foundation Model Alignment and Agent. <br /></p>
<h2>Experience</h2>
<ul>
<li><p>Applied Scientist Intern, Amazon Web Search (<b>AWS</b>) AI Research and Education lab, Santa Clara, May 2024 - Oct 2024. <br />
Design RLHF algorithm to align LLMs with high-quality demonstrations. (Mentor: <a href="http://yao-liu.com/">Yao Liu</a> and <a href="https://sites.google.com/site/rfakoor">Rasool Fakoor</a>) <br /></p>
</li>
</ul>
<ul>
<li><p>Applied Scientist Intern, Amazon Web Search (<b>AWS</b>) AI Research and Education lab, Santa Clara, May 2023 - Oct 2023. <br />
Design RLHF algorithm for LLM alignment with low compuational cost. (Mentor: <a href="https://kaixianglin.github.io/">Kaixiang Lin</a>) <br /></p>
</li>
</ul>
<h2>Selected Publications (* indicates equal contribution.)</h2>
<ul>
<li><p><a href="https://arxiv.org/pdf/2406.06874">Joint Demonstration and Preference Learning Improves Policy Alignment with Human Feedback</a> <br />
Chenliang Li, <b>Siliang Zeng</b>, Zeyi Liao, Jiaxiang Li, Dongyeop Kang, Alfredo Garcia, Mingyi Hong <br /> 
The Thirteenth International Conference on Learning Representations (<b>ICLR 2025</b>). <br />
Also accepted to <b>NeurIPS 2025 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability</b> (FITML 2025). <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2405.17888">Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment</a> <br /> 
Jiaxiang Li, <b>Siliang Zeng</b>, Hoi-To Wai, Chenliang Li, Alfredo Garcia, Mingyi Hong <br />
Thirty-eighth Annual Conference on Neural Information Processing Systems (<b>NeurIPS 2024</b>). <br />
Also accepted to ICML 2024 Workshop on Theoretical Foundations of Foundation Models. <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2302.07457">When Demonstrations Meet Generative World Models: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning</a> <br /> 
<b>Siliang Zeng*</b>, Chenliang Li*, Alfredo Garcia, Mingyi Hong <br />
Thirty-seventh Conference on Neural Information Processing Systems (<b>NeurIPS 2023</b>). (<b>Oral: 0.5% [67/12343] </b>) <br /></p>
</li>
</ul>
<ul>
<li><p><a href="http://arxiv.org/abs/2210.01282">Structural Estimation of Markov Decision Processes in High-Dimensional
State Space with Finite-Time Guarantees</a> <br /> 
<b>Siliang Zeng</b>, Mingyi Hong, Alfredo Garcia <br />
<b>Operations Research</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/forum?id=zbt3VmTsRIj&amp;referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1)">Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees</a> <br /> 
<b>Siliang Zeng</b>, Chenliang Li, Alfredo Garcia, Mingyi Hong <br />
Thirty-sixth Conference on Neural Information Processing Systems (<b>NeurIPS 2022</b>). <br />
(A previous version accepted by Decision Awareness in Reinforcement Learning Workshop at ICML 2022) </p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/forum?id=0RMDK39mGg&amp;referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1)">A Stochastic Linearized Augmented Lagrangian Method for Decentralized Bilevel Optimization</a> <br />
Songtao Lu, <b>Siliang Zeng</b>, Xiaodong Cui, Mark S. Squillante, Lior Horesh, Brian Kingsbury, Jia Liu, Mingyi Hong <br />
Thirty-sixth Conference on Neural Information Processing Systems (<b>NeurIPS 2022</b>). <br />
Selected as an <b>Honorable Mention</b> in the <b>IBM Pat Goldberg Memorial competition for best papers</b>. <br /></p>
</li>
</ul>
<h2>Recent News</h2>
<ul>
<li><p>Jan 2025: Two papers have been accepted to <b>ICLR 2025</b> and <b>AISTATS 2025</b>: <br />
<a href="https://arxiv.org/pdf/2406.06874">Joint Demonstration and Preference Learning Improves Policy Alignment with Human Feedback.</a> <br />
<a href="Understanding">Inverse Reinforcement Learning under Overparameterization: Non-Asymptotic Analysis and Global Optimality.</a> (To Appear in AISTATS 2025)</p>
</li>
</ul>
<ul>
<li><p>Oct 2024: Two papers got accepted to <b>NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability</b> (FITML 2024): <br />
<a href="https://openreview.net/forum?id=D8SSXvhZHl&amp;referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1)">Learning Reward and Policy Jointly from Demonstration and Preference Improves Alignment.</a> <br />
<a href="https://openreview.net/forum?id=AdNrerin75">Policy Optimization can be Memory-Efficient: LLM Alignment Through Successive Policy Re-weighting (SPR).</a></p>
</li>
</ul>
<ul>
<li><p>Sep 2024: One paper has been accepted to <b>NeurIPS 2024</b>: <br />
<a href="https://arxiv.org/abs/2405.17888">Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment</a></p>
</li>
</ul>
<ul>
<li><p>August 2024: Our paper titled &ldquo;A stochastic linearized augmented Lagrangian method for decentralized bilevel optimization&rdquo; 
has been <br /> selected as an <b>Honorable Mention</b> in the <b>IBM Pat Goldberg Memorial competition for best papers</b>!</p>
</li>
</ul>
<ul>
<li><p>July 2024: One paper has been accepted to <b>Operations Research</b>: <br /> 
<a href="https://arxiv.org/abs/2210.01282">Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees</a></p>
</li>
</ul>
<ul>
<li><p>Jun 2024: One paper has been accepted to <b>ICML 2024 Workshop on 
Theoretical Foundations of Foundation Models</b>: <br /> <a href="https://arxiv.org/abs/2405.17888">Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment</a></p>
</li>
</ul>
<ul>
<li><p>May 2024: I join <b>Amazon Web Search (AWS)</b> as an applied scientist intern. I will work on large language model alignment.</p>
</li>
</ul>
<ul>
<li><p>Apr 2024: I am thrilled to receive the <b>2024-25 Doctoral Dissertation Fellowship</b> (DDF). </p>
</li>
</ul>
<ul>
<li><p>Apr 2024: I am invited to give a talk at <b>Ai4</b> - Industry's Leading AI Conference. See you in Las Vegas!</p>
</li>
</ul>
<ul>
<li><p>Nov 2023: I am invited to give a talk at the Coordinated Science Laboratory of UIUC.</p>
</li>
</ul>
<ul>
<li><p>Oct 2023: I am thrilled to receive the <b>NeurIPS 2023 Scholar Award</b>. See you in New Orleans!</p>
</li>
</ul>
<ul>
<li><p>Sep 2023: One paper has been accepted to <b>NeurIPS 2023</b> as an <b>oral</b>: <a href="https://arxiv.org/abs/2302.07457"><br /> When Demonstrations Meet Generative World Models: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning</a>. <br /></p>
</li>
</ul>
<ul>
<li><p>August 2023: One paper has been accepted to <b>Conference on Robot Learning (CoRL) 2023</b>: <a href="https://openreview.net/forum?id=W5SrUCN0yUa&amp;referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1)"><br /> A Bayesian Approach to Robust Inverse Reinforcement Learning</a>. <br /></p>
</li>
</ul>
<ul>
<li><p>May 2023: I join <b>Amazon Web Search (AWS)</b> as an applied scientist intern. I will work on reinforcement learning and large language model training.</p>
</li>
</ul>
<ul>
<li><p>March 2023: I gave an invited talk at Amazon AWS about our recent work on offline inverse reinforcement learning: <br /> 
<a href="https://arxiv.org/abs/2302.07457">Understanding Expertise through Demonstrations: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning</a> <br /> </p>
</li>
</ul>
<ul>
<li><p>Oct 2022: I am thrilled to receive the <b>NeurIPS 2022 Scholar Award</b>. See you in New Orleans!</p>
</li>
</ul>
<ul>
<li><p>Sep 2022: Two papers have been accepted to <b>NeurIPS 2022</b>: <br />
<a href="https://openreview.net/forum?id=zbt3VmTsRIj&amp;referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1)">Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees.</a> <br />
<a href="https://openreview.net/attachment?id=0RMDK39mGg&amp;name=supplementary_material">A Stochastic Linearized Augmented Lagrangian Method for Decentralized Bilevel Optimization.</a> <br />




</p>
</li>
</ul>
<ul>
<li><p>Jul 2022: I have been selected for a travel grant from the DARL Workshop at ICML 2022! 
</p>
</li>
</ul>
<ul>
<li><p>Jun 2022: One paper has been accepted to <b>Decision Awareness in Reinforcement Learning Workshop at ICML 2022</b>: <a href="https://openreview.net/forum?id=FfELl5h3Nec"><br /> Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees</a>. <br />




</p>
</li>
</ul>
<ul>
<li><p>Jun 2022: One paper has been accepted to <b>SIAM Journal on Optimization</b>: <a href="https://arxiv.org/abs/2006.11662"><br /> On the Divergence of Decentralized Non-Convex Optimization</a>. <br />



</p>
</li>
</ul>
<ul>
<li><p>Mar 2022: One paper has been accepted to <b>L4DC 2022</b>: <a href="https://arxiv.org/abs/2110.05597"><br /> Learning to Coordinate in Multi-Agent Systems: A Coordinated Actor-Critic Algorithm and Finite-Time Guarantees</a>. <br />




</p>
</li>
</ul>
<ul>
<li><p>Sep 2021: One paper has been accepted to <b>NeurIPS 2021</b>: <a href="https://arxiv.org/abs/2102.07367"><br /> A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum</a>. <br />


</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
