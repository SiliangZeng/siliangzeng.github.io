<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Siliang Zeng</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">MENU</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="awards.html">Awards</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Siliang Zeng</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photo.jpg" alt="" width="200px" />&nbsp;</td>
<td align="left"><p><br /> <a href="https://cse.umn.edu/ece">Electrical and Computer Engineering</a>, <br />
<a href="https://twin-cities.umn.edu/">University of Minnesota, Twin Cities</a><br />
<a href="https://scholar.google.com/citations?user=IfqsDyYAAAAJ&amp;hl=en">Google Scholar</a>, <a href="https://www.dropbox.com/scl/fi/05podn8eq43qpjodyw56y/Siliang-CV.pdf?rlkey=28fsbv17ewtusw2zsw63q6cyz&amp;st=d30mp9dn&amp;dl=0">CV</a> <br />
Email: zeng0176 <a href="at">at</a> umn (dot) edu</p>
</td></tr></table>
<h2>About me</h2>
<p>I am a Research Scientist on the LLM Post-Training team at Bytedance Seed. Previously, I obtained my PhD at <a href="https://twin-cities.umn.edu/">University of Minnesota</a>, advised by <a href="https://people.ece.umn.edu/~mhong/mingyi.html">Prof. Mingyi Hong</a>. <br />
Before moving to Minnesota, I received the B.S. in Statistics from <a href="https://www.cuhk.edu.cn/en">The Chinese University of Hong Kong, Shenzhen</a> in 2020. <br /></p>
<p>In my research, I am always thinking about how to design practical algorithms and systems for training RL agents to solve sequential decision-making <br />  
problems under uncertainty. My recent research interests focus on Foundation Model Alignment and Agent. <br /></p>
<h2>Experience</h2>
<ul>
<li><p>Research Scientist, Bytedance Seed, San Jose, CA, Jun 2025 - Present. <br /></p>
</li>
</ul>
<ul>
<li><p>Research Intern, Morgan Stanley ML Research, New York, NY, Feb 2025 - Apr 2025. <br />
Conducted Research in Agentic RL and Reasoning Models. (Mentor: <a href="https://x.com/willccbb">Will Brown</a>) <br /></p>
</li>
</ul>
<ul>
<li><p>Applied Scientist Intern, Amazon Web Search, Santa Clara, May 2024 - Oct 2024. <br />
Design RLHF algorithm to align LLMs with high-quality demonstrations. (Mentor: <a href="http://yao-liu.com/">Yao Liu</a> and <a href="https://sites.google.com/site/rfakoor">Rasool Fakoor</a>) <br /></p>
</li>
</ul>
<ul>
<li><p>Applied Scientist Intern, Amazon Web Search, Santa Clara, May 2023 - Sep 2023. <br />
Design RLHF algorithm for LLM alignment with low compuational cost. (Mentor: <a href="https://kaixianglin.github.io/">Kaixiang Lin</a>) <br /></p>
</li>
</ul>
<h2>Recent News</h2>
<ul>
<li><p>Jun 2025: One paper is accepted to <b>ICML 2025 Workshop on Computer Use Agents</b>: <br />
<a href="https://arxiv.org/pdf/2505.11821">Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment.</a> <br /></p>
</li>
</ul>
<ul>
<li><p>May 2025: I passed my PhD defense at University of Minnesota! <br /></p>
</li>
</ul>
<table class="imgtable"><tr><td>
<img src="Defense.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"></td></tr></table>
<ul>
<li><p>Mar 2025: One paper is accepted to <b>Reasoning and Planning for LLMs workshop @ ICLR2025</b>: <br />
<a href="https://openreview.net/pdf?id=7ETrvtvRlU">Reinforcement Learning in Inference Time: A Perspective from Successive Policy Iterations.</a> <br /></p>
</li>
</ul>
<ul>
<li><p>Feb 2025: One paper is accepted to <b>ICLR 2025</b> as a <b>spotlight</b>: <br />
<a href="https://openreview.net/forum?id=VCbqXtS5YY&amp;referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1)">Joint Demonstration and Preference Learning Improves Policy Alignment with Human Feedback.</a> <br /></p>
</li>
</ul>
<ul>
<li><p>Jan 2025: One <b>US Patent</b> with IBM researchers has been granted: <br />
<a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=IfqsDyYAAAAJ&amp;sortby=pubdate&amp;citation_for_view=IfqsDyYAAAAJ:iH-uZ7U-co4C">BILEVEL DECENTRALIZED MULTI-AGENT LEARNING</a> <br /></p>
</li>
</ul>
<ul>
<li><p>Jan 2025: One paper has been accepted to <b>TMLR</b>: <br />
<a href="https://arxiv.org/abs/2410.14655">Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens</a> <br /></p>
</li>
</ul>
<ul>
<li><p>Jan 2025: Two papers have been accepted to <b>ICLR 2025</b> and <b>AISTATS 2025</b>: <br />
<a href="https://arxiv.org/pdf/2406.06874">Joint Demonstration and Preference Learning Improves Policy Alignment with Human Feedback.</a> <br />
<a href="https://openreview.net/forum?id=j4CbQGb0iF">Understanding Inverse Reinforcement Learning under Overparameterization: Non-Asymptotic Analysis and Global Optimality.</a></p>
</li>
</ul>
<ul>
<li><p>Oct 2024: Two papers got accepted to <b>NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability</b> (FITML 2024): <br />
<a href="https://openreview.net/forum?id=D8SSXvhZHl&amp;referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1)">Learning Reward and Policy Jointly from Demonstration and Preference Improves Alignment.</a> <br />
<a href="https://openreview.net/forum?id=AdNrerin75">Policy Optimization can be Memory-Efficient: LLM Alignment Through Successive Policy Re-weighting (SPR).</a></p>
</li>
</ul>
<ul>
<li><p>Sep 2024: One paper has been accepted to <b>NeurIPS 2024</b>: <br />
<a href="https://arxiv.org/abs/2405.17888">Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment</a></p>
</li>
</ul>
<ul>
<li><p>August 2024: Our paper titled &ldquo;A stochastic linearized augmented Lagrangian method for decentralized bilevel optimization&rdquo; 
has been <br /> selected as an <b>Honorable Mention</b> in the <b>IBM Pat Goldberg Memorial competition for best papers</b>!</p>
</li>
</ul>
<ul>
<li><p>July 2024: One paper has been accepted to <b>Operations Research</b>: <br /> 
<a href="https://arxiv.org/abs/2210.01282">Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees</a></p>
</li>
</ul>
<ul>
<li><p>Jun 2024: One paper has been accepted to <b>ICML 2024 Workshop on 
Theoretical Foundations of Foundation Models</b>: <br /> <a href="https://arxiv.org/abs/2405.17888">Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment</a></p>
</li>
</ul>
<ul>
<li><p>May 2024: I join <b>Amazon Web Search (AWS)</b> as an applied scientist intern. I will work on large language model alignment.</p>
</li>
</ul>
<ul>
<li><p>Apr 2024: I am thrilled to receive the <b>2024-25 Doctoral Dissertation Fellowship</b> (DDF). </p>
</li>
</ul>
<ul>
<li><p>Apr 2024: I am invited to give a talk at <b>Ai4</b> - Industry's Leading AI Conference. See you in Las Vegas!</p>
</li>
</ul>
<ul>
<li><p>Nov 2023: I am invited to give a talk at the Coordinated Science Laboratory of UIUC.</p>
</li>
</ul>
<ul>
<li><p>Oct 2023: I am thrilled to receive the <b>NeurIPS 2023 Scholar Award</b>. See you in New Orleans!</p>
</li>
</ul>
<ul>
<li><p>Sep 2023: One paper has been accepted to <b>NeurIPS 2023</b> as an <b>oral</b>: <a href="https://arxiv.org/abs/2302.07457"><br /> When Demonstrations Meet Generative World Models: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning</a>. <br /></p>
</li>
</ul>
<ul>
<li><p>August 2023: One paper has been accepted to <b>Conference on Robot Learning (CoRL) 2023</b>: <a href="https://openreview.net/forum?id=W5SrUCN0yUa&amp;referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1)"><br /> A Bayesian Approach to Robust Inverse Reinforcement Learning</a>. <br /></p>
</li>
</ul>
<ul>
<li><p>May 2023: I join <b>Amazon Web Search (AWS)</b> as an applied scientist intern. I will work on reinforcement learning and large language model training.</p>
</li>
</ul>
<ul>
<li><p>March 2023: I gave an invited talk at Amazon AWS about our recent work on offline inverse reinforcement learning: <br /> 
<a href="https://arxiv.org/abs/2302.07457">Understanding Expertise through Demonstrations: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning</a> <br /> </p>
</li>
</ul>
<ul>
<li><p>Oct 2022: I am thrilled to receive the <b>NeurIPS 2022 Scholar Award</b>. See you in New Orleans!</p>
</li>
</ul>
<ul>
<li><p>Sep 2022: Two papers have been accepted to <b>NeurIPS 2022</b>: <br />
<a href="https://openreview.net/forum?id=zbt3VmTsRIj&amp;referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1)">Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees.</a> <br />
<a href="https://openreview.net/attachment?id=0RMDK39mGg&amp;name=supplementary_material">A Stochastic Linearized Augmented Lagrangian Method for Decentralized Bilevel Optimization.</a> <br />




</p>
</li>
</ul>
<ul>
<li><p>Jul 2022: I have been selected for a travel grant from the DARL Workshop at ICML 2022! 
</p>
</li>
</ul>
<ul>
<li><p>Jun 2022: One paper has been accepted to <b>Decision Awareness in Reinforcement Learning Workshop at ICML 2022</b>: <a href="https://openreview.net/forum?id=FfELl5h3Nec"><br /> Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees</a>. <br />




</p>
</li>
</ul>
<ul>
<li><p>Jun 2022: One paper has been accepted to <b>SIAM Journal on Optimization</b>: <a href="https://arxiv.org/abs/2006.11662"><br /> On the Divergence of Decentralized Non-Convex Optimization</a>. <br />



</p>
</li>
</ul>
<ul>
<li><p>Mar 2022: One paper has been accepted to <b>L4DC 2022</b>: <a href="https://arxiv.org/abs/2110.05597"><br /> Learning to Coordinate in Multi-Agent Systems: A Coordinated Actor-Critic Algorithm and Finite-Time Guarantees</a>. <br />




</p>
</li>
</ul>
<ul>
<li><p>Sep 2021: One paper has been accepted to <b>NeurIPS 2021</b>: <a href="https://arxiv.org/abs/2102.07367"><br /> A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum</a>. <br />


</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
