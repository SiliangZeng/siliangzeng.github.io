# jemdoc: menu{MENU}{index.html}, nofooter  
==Siliang Zeng

~~~
{}{img_left}{photo.jpeg}{}{200px}{}
\n [https://cse.umn.edu/ece Electrical and Computer Engineering], \n
[https://twin-cities.umn.edu/ University of Minnesota, Twin Cities]\n
[https://scholar.google.com/citations?user=IfqsDyYAAAAJ&hl=en Google Scholar], [https://www.dropbox.com/scl/fi/1mbuy7f5h5blrnvc0ry2n/Siliang-CV.pdf?rlkey=90pe9y84i74gw1t83yl8j1j8f&st=gs98rwar&dl=0 CV] \n
Email: zeng0176 [at] umn (dot) edu
~~~

== About me
I am a fifth year PhD student at [https://twin-cities.umn.edu/ University of Minnesota], advised by [https://people.ece.umn.edu/~mhong/mingyi.html Prof. Mingyi Hong]. \n
Before moving to Minnesota, I received the B.S. in Statistics from [https://www.cuhk.edu.cn/en The Chinese University of Hong Kong, Shenzhen] in 2020. \n

In my research, I am always thinking about how to design practical algorithms and systems for sequential decision making under uncertainty. \n 
My recent research interests focus on Foundation Model Alignment and Agent. \n


== Experience

- Applied Scientist Intern, Amazon Web Search (*AWS*) AI Research and Education lab, Santa Clara, May 2024 - Oct 2024. \n
Design RLHF algorithm to align LLMs with high-quality demonstrations. (Mentor: [http://yao-liu.com/ Yao Liu] and [https://sites.google.com/site/rfakoor Rasool Fakoor]) \n

- Applied Scientist Intern, Amazon Web Search (*AWS*) AI Research and Education lab, Santa Clara, May 2023 - Oct 2023. \n
Design RLHF algorithm for LLM alignment with low compuational cost. (Mentor: [https://kaixianglin.github.io/ Kaixiang Lin]) \n



== Selected Publications (\* indicates equal contribution.)

- [https://arxiv.org/abs/2405.17888 Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment] \n 
    Jiaxiang Li, *Siliang Zeng*, Hoi-To Wai, Chenliang Li, Alfredo Garcia, Mingyi Hong \n
    Thirty-eighth Annual Conference on Neural Information Processing Systems (*NeurIPS 2024*). \n
    Also accepted to ICML 2024 Workshop on Theoretical Foundations of Foundation Models. \n

- [https://arxiv.org/abs/2302.07457 When Demonstrations Meet Generative World Models: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning] \n 
    *Siliang Zeng\**, Chenliang Li\*, Alfredo Garcia, Mingyi Hong \n
    Thirty-seventh Conference on Neural Information Processing Systems (*NeurIPS 2023*). (*Oral: 0.5% \[67/12343\] *) \n


- [http://arxiv.org/abs/2210.01282 Structural Estimation of Markov Decision Processes in High-Dimensional
  State Space with Finite-Time Guarantees] \n 
    *Siliang Zeng*, Mingyi Hong, Alfredo Garcia \n
    *Operations Research*.

- [https://openreview.net/forum?id=zbt3VmTsRIj&referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1) Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees] \n 
    *Siliang Zeng*, Chenliang Li, Alfredo Garcia, Mingyi Hong \n
    Thirty-sixth Conference on Neural Information Processing Systems (*NeurIPS 2022*). \n
    (A previous version accepted by Decision Awareness in Reinforcement Learning Workshop at ICML 2022) 

- [https://openreview.net/forum?id=0RMDK39mGg&referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1) A Stochastic Linearized Augmented Lagrangian Method for Decentralized Bilevel Optimization] \n
    Songtao Lu, *Siliang Zeng*, Xiaodong Cui, Mark S. Squillante, Lior Horesh, Brian Kingsbury, Jia Liu, Mingyi Hong \n
    Thirty-sixth Conference on Neural Information Processing Systems (*NeurIPS 2022*). \n
    Selected as an *Honorable Mention* in the *IBM Pat Goldberg Memorial competition for best papers*. \n
    \n


== Recent News

- Oct 2024: Two papers got accepted to *NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability* (FITML 2024): \n
[https://openreview.net/forum?id=D8SSXvhZHl&referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1) Learning Reward and Policy Jointly from Demonstration and Preference Improves Alignment.] \n
[https://openreview.net/forum?id=AdNrerin75 Policy Optimization can be Memory-Efficient: LLM Alignment Through Successive Policy Re-weighting (SPR).]

- Sep 2024: One paper has been accepted to *NeurIPS 2024*: \n
[https://arxiv.org/abs/2405.17888 Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment]

- August 2024: Our paper titled "A stochastic linearized augmented Lagrangian method for decentralized bilevel optimization" 
has been \n selected as an *Honorable Mention* in the *IBM Pat Goldberg Memorial competition for best papers*!

- July 2024: One paper has been accepted to *Operations Research*: \n 
[https://arxiv.org/abs/2210.01282 Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees]

- Jun 2024: One paper has been accepted to *ICML 2024 Workshop on 
Theoretical Foundations of Foundation Models*: \n [https://arxiv.org/abs/2405.17888 Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment]

- May 2024: I join *Amazon Web Search (AWS)* as an applied scientist intern. I will work on large language model alignment.

- Apr 2024: I am thrilled to receive the *2024-25 Doctoral Dissertation Fellowship* (DDF). 

- Apr 2024: I am invited to give a talk at *Ai4* - Industry's Leading AI Conference. See you in Las Vegas!

- Nov 2023: I am invited to give a talk at the Coordinated Science Laboratory of UIUC.

- Oct 2023: I am thrilled to receive the *NeurIPS 2023 Scholar Award*. See you in New Orleans!

- Sep 2023: One paper has been accepted to *NeurIPS 2023* as an *oral*: [https://arxiv.org/abs/2302.07457 \n When Demonstrations Meet Generative World Models: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning]. \n

- August 2023: One paper has been accepted to *Conference on Robot Learning (CoRL) 2023*: [https://openreview.net/forum?id=W5SrUCN0yUa&referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1) \n A Bayesian Approach to Robust Inverse Reinforcement Learning ]. \n

- May 2023: I join *Amazon Web Search (AWS)* as an applied scientist intern. I will work on reinforcement learning and large language model training.

- March 2023: I gave an invited talk at Amazon AWS about our recent work on offline inverse reinforcement learning: \n 
    [https://arxiv.org/abs/2302.07457 Understanding Expertise through Demonstrations: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning] \n 

- Oct 2022: I am thrilled to receive the *NeurIPS 2022 Scholar Award*. See you in New Orleans!

- Sep 2022: Two papers have been accepted to *NeurIPS 2022*: \n
    [https://openreview.net/forum?id=zbt3VmTsRIj&referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1) Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees.] \n
    [https://openreview.net/attachment?id=0RMDK39mGg&name=supplementary_material A Stochastic Linearized Augmented Lagrangian Method for Decentralized Bilevel Optimization.] \n
    #\n
    #This paper develops an inverse reinforcement learning framework, which aims to recover the nonlinear reward function through maximizing the Likelihood function over the expert trajectories. 
    #A novel single-loop algorithm for IRL is proposed, where each policy improvement step is followed by a stochastic gradient step for likelihood maximization. We show that the proposed algorithm provably converges to a stationary solution with a finite-time guarantee. 
    #Finally, by using robotics control problems in Mujoco and their transfer settings, we show that the proposed algorithm achieves superior performance compared with other IRL and imitation learning benchmarks. \n
    #\n

- Jul 2022: I have been selected for a travel grant from the DARL Workshop at ICML 2022! 
    #\n

- Jun 2022: One paper has been accepted to *Decision Awareness in Reinforcement Learning Workshop at ICML 2022*: [https://openreview.net/forum?id=FfELl5h3Nec \n Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees]. \n
    #\n
    #This paper develops an inverse reinforcement learning framework, which aims to recover the nonlinear reward function through maximizing the Likelihood function over the expert trajectories. 
    #A novel single-loop algorithm for IRL is proposed, where each policy improvement step is followed by a stochastic gradient step for likelihood maximization. We show that the proposed algorithm provably converges to a stationary solution with a finite-time guarantee. 
    #Finally, by using robotics control problems in Mujoco and their transfer settings, we show that the proposed algorithm achieves superior performance compared with other IRL and imitation learning benchmarks. \n
    #\n

- Jun 2022: One paper has been accepted to *SIAM Journal on Optimization*: [https://arxiv.org/abs/2006.11662 \n On the Divergence of Decentralized Non-Convex Optimization]. \n
    #\n
    #We study a generic class of decentralized algorithms in which N agents jointly optimize sum local objectives. By constructing some counter-examples, we show that when certain local Lipschitz conditions (LLC) on the local function gradients are not satisfied, 
    #most of the existing decentralized algorithms diverge, even if the global Lipschitz condition (GLC) is satisfied, where the sum function f has Lipschitz gradient. We then design a first-order algorithm, which is capable of computing stationary solutions with neither the LLC nor the GLC. \n
    #\n

- Mar 2022: One paper has been accepted to *L4DC 2022*: [https://arxiv.org/abs/2110.05597 \n Learning to Coordinate in Multi-Agent Systems: A Coordinated Actor-Critic Algorithm and Finite-Time Guarantees]. \n
    #\n
    #This paper develops a novel collaboration mechanism for designing robust MARL systems with theoretical guarantees. Specifically, we propose a class of coordinated actor-critic (*CAC*) algorithms where agents partially share their individually parametrized policies with neighbors.
    #We  conduct extensive numerical experiments and present theoretical analysis to demonstrate the
    #effectiveness of the proposed algorithm.\n
    #\n

- Sep 2021: One paper has been accepted to *NeurIPS 2021*: [https://arxiv.org/abs/2102.07367 \n A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum]. \n
    #\n
    #This paper proposes a new algorithm – the Single-timescale Double-momentum Stochastic Approximation (*SUSTAIN*) – for tackling stochastic unconstrained bilevel optimization problems. 
    #The proposed algorithm achieves the best sample complexity for certain class of bi-level optimization problem.

